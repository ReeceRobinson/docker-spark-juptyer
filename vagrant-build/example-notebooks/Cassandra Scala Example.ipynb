{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassandra datasource example with Spark 1.6\n",
    "The following notebook shows how to use the Cassandra datasource connector from Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import com.datastax.spark.connector._\n",
    "import com.datastax.spark.connector.cql._\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.sql.SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to stop the default SparkContext (sc) and create one configured for Cassandra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Spark to use the Cassandra connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val conf = (new SparkConf(true)\n",
    "            .setMaster(\"local[*]\")\n",
    "            .setAppName(\"Jupyter Scala\")\n",
    "            .set(\"spark.cassandra.connection.host\", \"cassandra\")\n",
    "            .set(\"spark.cassandra.input.consistency.level\", \"LOCAL_ONE\")\n",
    "            )\n",
    "val sc = new SparkContext(conf)\n",
    "val sqlContext = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the test keyspace and table and insert a test row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResultSet[ exhausted: true, Columns[]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CassandraConnector(conf).withSessionDo { session =>\n",
    "                                        session.execute(\"CREATE KEYSPACE IF NOT EXISTS test WITH replication={'class':'SimpleStrategy', 'replication_factor':1}\")\n",
    "                                        session.execute(\"CREATE TABLE IF NOT EXISTS test.users (username text PRIMARY KEY, emails SET<text>)\")\n",
    "                                        session.execute(\"INSERT INTO test.users (username,emails) VALUES('someone',{'someone@email.com', 's@email.com'})\")\n",
    "                                       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the test data from Cassandra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").options(Map(\"cluster\" -> \"Cluster Dock\", \"table\" -> \"users\", \"keyspace\" -> \"test\")).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|username|              emails|\n",
      "+--------+--------------------+\n",
      "| someone|[s@email.com, som...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10 (Spark 1.6)",
   "language": "scala",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
